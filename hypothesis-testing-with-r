#-------------------------------------introduction-------------------------------------

Statistical hypothesis testing is a process that allows you to evaluate if a change or difference seen in a dataset is “real”, 
or if it’s just a result of random fluctuation in the data.

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r message = FALSE}
# load data
load("week_1.Rda")
load("week_2.Rda")
```

```{r}
# calculate week_1_mean and week_2_mean:
week_1_mean <- mean(week_1)
week_1_mean
week_2_mean <- mean(week_2)
week_2_mean
```

```{r}
# calculate week_1_sd and week_2_sd:
week_1_sd <- sd(week_1)
week_1_sd
week_2_sd <- sd(week_2)
week_2_sd
```

```{r}
# run two sample t-test:
results <- t.test(week_1,week_2)
results
```
  
#-------------------------------------sample mean and population mean I-------------------------------------

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r}
# generate random population
population <- rnorm(300, mean=65, sd=3.5)

# calculate population mean here:
population_mean <- mean(population)
population_mean
```

```{r}
# generate sample 1
sample_1 <- sample(population, size=30)
sample_1

# calculate sample 1 mean
sample_1_mean <- mean(sample_1)
sample_1_mean
```

```{r}
# generate samples 2,3,4 and 5
sample_2 <- sample(population, size=30)
sample_3 <- sample(population, size=30)
sample_4 <- sample(population, size=30)
sample_5 <- sample(population, size=30)
```

```{r}
# calculate sample means here:
sample_2_mean <- mean(sample_2)
sample_2_mean
sample_3_mean <- mean(sample_3)
sample_3_mean
sample_4_mean <- mean(sample_4)
sample_4_mean
sample_5_mean <- mean(sample_5)
sample_5_mean
```

#-------------------------------------sample mean and population mean II-------------------------------------

The random sample of 20 students was skewed to one direction of the total population. 
More 11 year olds were chosen in the sample than is representative of the whole school, bringing down the average height of the sample. 
This is called a sampling error, and occurs when a sample is not representative of the population it comes from.

#-------------------------------------hypothesis formulation-------------------------------------

A null hypothesis states that there is no difference between the populations you are comparing, 
and it implies that any difference seen in the sample data is due to sampling error.

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r}
# experiment 1
hypo_a <- "DeePressurize lowers blood pressure in patients."
hypo_b <- "DeePressurize has no effect on blood pressure in patients."
null_hypo_1 <- "hypo_b"
```

```{r}
# experiment 2
hypo_c <- "The new profile layout has no effect on number of matches with other users."
hypo_d <- "The new profile layout results in more matches with other users than the original layout."
null_hypo_2 <- "hypo_c"
```
  
#-------------------------------------designing an experiment-------------------------------------

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r}
# load data
load("retriever_lengths.Rda")
load("doodle_lengths.Rda")
```

```{r}
# calculate mean_retriever_l and mean_doodle_l here:
mean_retriever_l <- mean(retriever_lengths)
mean_doodle_l <- mean(doodle_lengths)
mean_retriever_l
mean_doodle_l
```

```{r}
# calculate mean_difference here:
mean_difference <- mean_retriever_l-mean_doodle_l
mean_difference
```

```{r}
# statements:
st_1 <- "The average length of Golden Retrievers is 2.5 inches longer than the average length of Goldendoodles."
st_2 <- "The average length of Golden Retrievers is the same as the average length of Goldendoodles."

# update null_hypo here:
null_hypo <- "st_2"
```

#-------------------------------------type I and type II errors-------------------------------------

A Type I error occurs when a hypothesis test finds a correlation between things that are not related. 
This error is sometimes called a “false positive” and occurs when the null hypothesis is rejected even though it is true.

A Type II error, is failing to find a correlation between things that are actually related. 
This error is referred to as a “false negative” and occurs when the null hypothesis is not rejected even though it is false.

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r}
# the true positives and negatives:
actual_positive <- c(2, 5, 6, 7, 8, 10, 18, 21, 24, 25, 29, 30, 32, 33, 38, 39, 42, 44, 45, 47)
actual_negative <- c(1, 3, 4, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 22, 23, 26, 27, 28, 31, 34, 35, 36, 37, 40, 41, 43, 46, 48, 49)

# the positives and negatives we determine by running the experiment:
experimental_positive <- c(2, 4, 5, 7, 8, 9, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 32, 35, 36, 38, 39, 40, 45, 46, 49)
experimental_negative <- c(1, 3, 6, 12, 14, 23, 25, 29, 30, 31, 33, 34, 37, 41, 42, 43, 44, 47, 48)
```

```{r}
# define type_i_errors and type_ii_errors here:
type_i_errors <- intersect(actual_negative, experimental_positive)
type_i_errors
type_ii_errors <- intersect(actual_positive, experimental_negative)
type_ii_errors
```

#-------------------------------------p-values-------------------------------------

P-values help determine how confident you can be in validating the null hypothesis. 
In this context, a p-value is the probability that, assuming the null hypothesis is true, 
you would see at least such a difference in the sample means of your data.

Consider the experiment on history and chemistry majors and their interest in volleyball from a previous exercise:

Null Hypothesis: "History and chemistry students are interested in volleyball at the same rates"
Experiment Sample Means: 34% of history majors and 39% of chemistry majors sign up for the volleyball class

Assuming the null hypothesis is true, there is no actual difference in preference for volleyball between all history and chemistry majors, 
and any difference present in the experiment data is the result of sampling error. Imagine you run a hypothesis test on this experiment data 
and it returns a p-value of 0.04. A p-value of 0.04 indicates that you could expect to see a difference of at least 5% (calculated as 39% - 34% = 5%) 
in the sample means only 4% of the time. Essentially, if you ran this same experiment 100 times, you would expect to see as large a difference 
in the sample means only 4 times given the assumption that there is no actual difference between the populations (i.e. they have the same mean).

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r}
# possible interpretations
st_1 <- "There is a 20% chance that the difference in average weight of green and red apples is due to random sampling."
st_2 <- "There is a 20% chance that green and red apples have the same average weight."
st_3 <- "There is a 20% chance red apples weigh more than green apples."
st_4 <- "There is a 20% chance green apples weigh more than green apples."

# update the value of interpretation here:
interpretation <- "st_1"
```

#-------------------------------------significance level-------------------------------------

While a hypothesis test will return a p-value indicating a level of confidence in the null hypothesis, it does not definitively claim 
whether you should reject the null hypothesis. To make this decision, you need to determine a threshold p-value for which all p-values below it 
will result in rejecting the null hypothesis. This threshold is known as the significance level. A higher significance level is more likely to 
give a false positive, as it makes it “easier” to state that there is a difference in the populations of your data when such a difference might not actually exist. 
If you want to be very sure that the result is not due to sampling error, you should select a very small significance level. It is important to choose 
the significance level before you perform a statistical hypothesis test. If you wait until after you receive a p-value from a test, you might pick a 
significance level such that you get the result you want to see. For instance, if someone is trying to publish the results of their scientific study in a journal, 
they might set a higher significance level that makes their results appear statistically significant. Choosing a significance level in advance helps keep everyone honest.
It is an industry-standard to set a significance level of 0.05 or less, meaning that there is a 5% or less chance that your result is due to sampling error.

#-------------------------------------one sample t-test-------------------------------------

A One Sample T-Test compares a sample mean to a hypothetical population mean. 
It answers the question “What is the probability that the sample came from a distribution with the desired mean?”

One result of a One Sample T-Test will be a p-value, which tells you whether or not you can reject this null hypothesis. 
If the p-value you receive is less than your significance level, normally 0.05, you can reject the null hypothesis and state that there is 
a significant difference.

R has a function called t.test() in the stats package which can perform a One Sample T-Test for you.
t.test() requires two arguments, a distribution of values and an expected mean:

results <- t.test(sample_distribution, mu = expected_mean)

sample_distribution is the sample of values that were collected
mu is an argument indicating the desired mean of the hypothetical population
expected_mean is the value of the desired mean
t.test() will return, among other information we will not cover here, a p-value — this tells you how confident you can be that 
the sample of values came from a distribution with the specified mean.

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r message = FALSE}
# load and view data
load("ages.Rda")
ages
```

```{r}
# calculate ages_mean here:
ages_mean <- mean(ages)
ages_mean
```

```{r}
# perform t-test here:
results <- t.test(ages, mu = 30)
results
```

#-------------------------------------two sample t-test-------------------------------------

Two Sample T-Test compares two sets of data, which are both approximately normally distributed.
You can use R’s t.test() function to perform a Two Sample T-Test, as shown below:

results <- t.test(distribution_1, distribution_2)

When performing a Two Sample T-Test, t.test() takes two distributions as arguments and returns, among other information, a p-value. 
Remember, the p-value let’s you know the probability that the difference in the means happened by chance (sampling error).

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r message = FALSE}
# load data
load("week_1.Rda")
week_1
load("week_2.Rda")
week_2
```

```{r}
# calculate week_1_mean and week_2_mean here:
week_1_mean <- mean(week_1)
week_2_mean <- mean(week_2)
week_1_mean
week_2_mean
```

```{r}
# calculate week_1_sd and week_2_sd here:
week_1_sd <- sd(week_1)
week_2_sd <- sd(week_2)
week_1_sd
week_2_sd
```

```{r}
# run two sample t-test here:
results <- t.test(week_1, week_2)
results
```

#-------------------------------------dangers of multiple t-tests-------------------------------------

The more t-tests you perform, the more likely that you are to get a false positive, a Type I error.

For a p-value of 0.05, if the null hypothesis is true, then the probability of obtaining a significant result is 1 – 0.05 = 0.95. 
When you run another t-test, the probability of still getting a correct result is 0.95 * 0.95, or 0.9025. 
That means your probability of making an error is now close to 10%! This error probability only gets bigger with the more t-tests you do.

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r message = FALSE}
# load data
load("store_a.Rda")
load("store_b.Rda")
load("store_c.Rda")
```

```{r}
# calculate means here:
store_a_mean <- mean(store_a)
store_b_mean <- mean(store_b)
store_c_mean <- mean(store_c)
store_a_mean
store_b_mean
store_c_mean
```

```{r}
# calculate standard deviations here:
store_a_sd <- sd(store_a)
store_b_sd <- sd(store_b)
store_c_sd <- sd(store_c)
store_a_sd
store_b_sd
store_c_sd
```

```{r}
# perform two sample t-test here:
a_b_results <- t.test(store_a, store_b)
a_c_results <- t.test(store_a, store_c)
b_c_results <- t.test(store_b, store_c)
a_b_results
a_c_results
b_c_results
```{r}
# calculate error_prob here:
error_prob <- (1-0.95**3)
error_prob
```

#-------------------------------------anova-------------------------------------

When comparing more than two numerical datasets, the best way to preserve a Type I error probability of 0.05 is to use ANOVA. 
ANOVA (Analysis of Variance) tests the null hypothesis that all of the datasets you are considering have the same mean. 
If you reject the null hypothesis with ANOVA, you’re saying that at least one of the sets has a different mean; 
however, it does not tell you which datasets are different.

You can use the stats package function aov() to perform ANOVA on multiple datasets. 
aov() takes the different datasets combined into a data frame as an argument. 
For example, if you were comparing scores on a video game between math majors, writing majors, and psychology majors, 
you could format the data in a data frame df_scores as follows:

group	        score
math major	  88
math major	  81
writing major	92
writing major	80
psychology major	94
psychology major	83

You can then run an ANOVA test with this line:

results <- aov(score ~ group, data = df_scores)

Note: score ~ group indicates the relationship you want to analyze (i.e. how each group, or major, relates to score on the video game)
To retrieve the p-value from the results of calling aov(), use the summary() function:

summary(results)

The null hypothesis, in this case, is that all three populations have the same mean score on this video game. 
If you reject this null hypothesis (if the p-value is less than 0.05), you can say you are reasonably confident that a pair of datasets is significantly different. 
After using only ANOVA, however, you can’t make any conclusions on which two populations have a significant difference.

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r message = FALSE}
# load libraries
library(tidyr)
```

```{r message = FALSE}
# load data
load("stores.Rda")
load("stores_new.Rda")
```

```{r}
# inspect stores here:
head(stores)
```

```{r}
# perform anova on stores here:
results <- aov(sales ~ store, data = stores)
summary(results)
```

```{r}
# perform anova on stores_new here:
results_new <- aov(sales ~ store, data = stores_new)
summary(results_new)
```

#-------------------------------------assumptions of numerical hypothesis tests-------------------------------------

1. The samples should each be normally distributed…ish
2. The population standard deviations of the groups should be equal
3. The samples must be independent

---
title: "Hypothesis Testing in R"
output: html_notebook
---

```{r message = FALSE}
# load data
load("dist_one.Rda")
load("dist_two.Rda")
load("dist_three.Rda")
load("dist_four.Rda")
```

```{r}
# plot histograms and define not_normal here:
hist(dist_one)
hist(dist_two)
hist(dist_three)
hist(dist_four)
not_normal <- 4
```{r}
# define ratio here:
ratio <- sd(dist_two)/sd(dist_three)
ratio
```

#-------------------------------------review-------------------------------------

Samples are subsets of an entire population, and the sample mean can be used to approximate the population mean
The null hypothesis is an assumption that there is no difference between the populations you are comparing in a hypothesis test
Type I Errors occur when a hypothesis test finds a correlation between things that are not related
Type II Errors occur when a hypothesis test fails to find a correlation between things that are actually related
P-Values indicate the probability that, assuming the null hypothesis is true, such differences in the samples you are comparing would exist
The Significance Level is a threshold p-value for which all p-values below it will result in rejecting the null hypothesis
One Sample T-Tests indicate whether a dataset belongs to a distribution with a given mean
Two Sample T-Tests indicate whether there is a significant difference between two datasets
ANOVA (Analysis of Variance) allows you to detect if there is a significant difference between one of multiple datasets
